name: "Terraform Destroy"
on:
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment to destroy"
        required: true
        default: "development"
        type: choice
        options:
          - development
          - staging
          - production
      confirm_destroy:
        description: 'Type "destroy" to confirm'
        required: true
        type: string

permissions:
  contents: read
  pull-requests: write

env:
  AWS_REGION: "us-west-2"
  TF_LOG: info
  TERRAFORM_WORKING_DIR: "src/terraform"

jobs:
  terraform-destroy:
    name: "Terraform Destroy"
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment }}
    if: github.event.inputs.confirm_destroy == 'destroy'
    concurrency:
      group: ${{ github.workflow }}-${{ github.event.inputs.environment }}
      cancel-in-progress: false

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Verify AWS Credentials
        run: aws sts get-caller-identity

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.5.0"
          terraform_wrapper: false

      - name: Terraform Init
        id: init
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: |
          terraform init \
            -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
            -backend-config="key=${{ github.event.inputs.environment }}/terraform.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}"

      - name: Configure kubectl for cleanup
        run: |
          aws eks update-kubeconfig \
            --name airflow-${{ github.event.inputs.environment }} \
            --region ${{ env.AWS_REGION }}
        continue-on-error: true

      - name: Clean EKS Resources
        run: |
          # Try to drain and delete nodes if possible
          NODES=$(kubectl get nodes -o name || echo "")
          if [ ! -z "$NODES" ]; then
            echo "Draining nodes: $NODES"
            for node in $NODES; do
              kubectl drain $node --ignore-daemonsets --delete-emptydir-data --force --timeout=2m || true
            done
          fi

          # Delete all workloads first
          kubectl delete deploy,sts,ds --all -A --timeout=5m || true
          kubectl delete pvc,pv --all -A --timeout=5m || true

          # Clean up kubectl config
          rm -rf ~/.kube/config ~/.kube/cache || true
        continue-on-error: true

      - name: Clean EKS Node Groups
        run: |
          CLUSTER_NAME="airflow-${{ github.event.inputs.environment }}"

          echo "Checking node groups for cluster $CLUSTER_NAME..."
          NODEGROUPS=$(aws eks list-nodegroups \
            --cluster-name $CLUSTER_NAME \
            --region ${{ env.AWS_REGION }} \
            --query 'nodegroups[*]' \
            --output text || echo "")

          if [ ! -z "$NODEGROUPS" ]; then
            echo "Found node groups: $NODEGROUPS"
            for ng in $NODEGROUPS; do
              echo "Deleting node group: $ng"
              aws eks delete-nodegroup \
                --cluster-name $CLUSTER_NAME \
                --nodegroup-name $ng \
                --region ${{ env.AWS_REGION }} || true
              
              echo "Waiting for node group deletion..."
              while aws eks describe-nodegroup \
                --cluster-name $CLUSTER_NAME \
                --nodegroup-name $ng \
                --region ${{ env.AWS_REGION }} 2>/dev/null; do
                echo "Still waiting for node group $ng to be deleted..."
                sleep 30
              done
            done
          fi
        continue-on-error: true

      - name: Wait for Resource Cleanup
        run: sleep 60

      - name: Terraform Destroy
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: |
          # Refresh state before destroy
          terraform refresh || true

          # Main destroy attempt
          terraform destroy -auto-approve -input=false \
            -var="environment=${{ github.event.inputs.environment }}"

      - name: Cleanup Terraform State
        if: success()
        run: |
          aws s3 rm "s3://${{ secrets.TF_STATE_BUCKET }}/${{ github.event.inputs.environment }}/terraform.tfstate"

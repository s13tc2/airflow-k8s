name: "Terraform Apply"
on:
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment to apply to"
        required: true
        default: "production"
        type: choice
        options:
          - production
          - staging
          - development
      confirm_apply:
        description: 'Type "apply" to confirm'
        required: true
        type: string

permissions:
  contents: read
  pull-requests: write

env:
  AWS_REGION: "us-west-2"
  TF_LOG: info
  TERRAFORM_WORKING_DIR: "./src/terraform"
  CLUSTER_NAME: "airflow-cluster"

jobs:
  terraform-apply:
    name: "Terraform Apply"
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment }}
    if: github.event.inputs.confirm_apply == 'apply'

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.5.0"
          terraform_wrapper: false

      - name: Clean Previous State
        run: |
          rm -rf .terraform .terraform.lock.hcl || true
          rm -rf ~/.kube/config ~/.kube/cache || true

      - name: Check and Clean Existing IAM Roles
        run: |
          # Function to detach and delete role policies
          delete_role() {
            local ROLE_NAME=$1
            echo "Checking role: $ROLE_NAME"
            
            # List and detach attached policies
            for ARN in $(aws iam list-attached-role-policies --role-name $ROLE_NAME --query 'AttachedPolicies[*].PolicyArn' --output text 2>/dev/null); do
              echo "Detaching policy: $ARN from role: $ROLE_NAME"
              aws iam detach-role-policy --role-name $ROLE_NAME --policy-arn $ARN
            done
            
            # Delete the role
            echo "Deleting role: $ROLE_NAME"
            aws iam delete-role --role-name $ROLE_NAME
          }

          # Delete cluster role
          CLUSTER_ROLE="${CLUSTER_NAME}-cluster-role"
          if aws iam get-role --role-name $CLUSTER_ROLE 2>/dev/null; then
            delete_role $CLUSTER_ROLE
          fi

          # Delete node role
          NODE_ROLE="${CLUSTER_NAME}-node-role"
          if aws iam get-role --role-name $NODE_ROLE 2>/dev/null; then
            delete_role $NODE_ROLE
          fi
        continue-on-error: true

      - name: Check and Clean Existing Resources
        run: |
          # Check for existing node groups
          if NODEGROUPS=$(aws eks list-nodegroups --cluster-name $CLUSTER_NAME --region ${{ env.AWS_REGION }} --query 'nodegroups[*]' --output text 2>/dev/null); then
            echo "Found existing node groups: $NODEGROUPS"
            for ng in $NODEGROUPS; do
              echo "Deleting node group: $ng"
              aws eks delete-nodegroup \
                --cluster-name $CLUSTER_NAME \
                --nodegroup-name $ng \
                --region ${{ env.AWS_REGION }}
              
              echo "Waiting for node group deletion..."
              aws eks wait nodegroup-deleted \
                --cluster-name $CLUSTER_NAME \
                --nodegroup-name $ng \
                --region ${{ env.AWS_REGION }}
            done
          fi
        continue-on-error: true

      - name: Terraform Init
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: |
          terraform init \
            -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
            -backend-config="key=${{ github.event.inputs.environment }}/terraform.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}"

      - name: Terraform Plan
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: |
          terraform plan \
            -var="environment=${{ github.event.inputs.environment }}" \
            -var="cluster_name=$CLUSTER_NAME" \
            -out=tfplan

      - name: Terraform Apply
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: |
          terraform apply -auto-approve -input=false tfplan

      - name: Verify Cluster
        if: success()
        run: |
          echo "Waiting for cluster to be ready..."
          aws eks wait cluster-active \
            --name $CLUSTER_NAME \
            --region ${{ env.AWS_REGION }}

          echo "Configuring kubectl..."
          aws eks update-kubeconfig \
            --name $CLUSTER_NAME \
            --region ${{ env.AWS_REGION }}

          echo "Waiting for nodes to be ready..."
          kubectl wait --for=condition=ready nodes --all --timeout=10m

      - name: Output Cluster Info
        if: success()
        run: |
          echo "Cluster Info:"
          kubectl cluster-info

          echo "Node Status:"
          kubectl get nodes

          echo "Airflow Status:"
          kubectl get pods -n airflow
          echo "Airflow Webserver URL:"
          kubectl get svc -n airflow airflow-webserver

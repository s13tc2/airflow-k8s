name: "Terraform Apply"
on:
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment to apply to"
        required: true
        default: "production"
        type: choice
        options:
          - production
          - staging
          - development
      confirm_apply:
        description: 'Type "apply" to confirm'
        required: true
        type: string

permissions:
  contents: read
  pull-requests: write

env:
  AWS_REGION: "us-west-2"
  TF_LOG: info
  TERRAFORM_WORKING_DIR: "./src/terraform"
  CLUSTER_NAME: "airflow-cluster"

jobs:
  terraform-apply:
    name: "Terraform Apply"
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment }}
    if: github.event.inputs.confirm_apply == 'apply'

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.5.0"
          terraform_wrapper: false

      - name: Clean Previous State
        run: |
          rm -rf .terraform .terraform.lock.hcl || true
          rm -rf ~/.kube/config ~/.kube/cache || true

      - name: Check and Clean Existing VPC Resources
        run: |
          # Function to delete NAT Gateways
          delete_nat_gateways() {
            local VPC_ID=$1
            echo "Checking NAT Gateways in VPC: $VPC_ID"
            NAT_GATEWAYS=$(aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$VPC_ID" --query 'NatGateways[*].NatGatewayId' --output text)
            for ng in $NAT_GATEWAYS; do
              echo "Deleting NAT Gateway: $ng"
              aws ec2 delete-nat-gateway --nat-gateway-id $ng
              echo "Waiting for NAT Gateway deletion..."
              aws ec2 wait nat-gateway-deleted --nat-gateway-ids $ng
            done
          }

          # Function to delete EIPs
          delete_eips() {
            echo "Checking Elastic IPs"
            EIPS=$(aws ec2 describe-addresses --query 'Addresses[*].AllocationId' --output text)
            for eip in $EIPS; do
              echo "Deleting EIP: $eip"
              aws ec2 release-address --allocation-id $eip
            done
          }

          # Function to clean VPC resources
          clean_vpc() {
            local VPC_ID=$1
            echo "Cleaning VPC: $VPC_ID"

            # Delete NAT Gateways first
            delete_nat_gateways $VPC_ID

            # Delete EIPs
            delete_eips

            # Delete Load Balancers
            echo "Checking for Load Balancers"
            LBS=$(aws elbv2 describe-load-balancers --query 'LoadBalancers[?VpcId==`'$VPC_ID'`].LoadBalancerArn' --output text)
            for lb in $LBS; do
              echo "Deleting Load Balancer: $lb"
              aws elbv2 delete-load-balancer --load-balancer-arn $lb
            done

            # Detach and delete Internet Gateway
            echo "Checking Internet Gateways"
            IGW=$(aws ec2 describe-internet-gateways --filters "Name=attachment.vpc-id,Values=$VPC_ID" --query 'InternetGateways[*].InternetGatewayId' --output text)
            if [ ! -z "$IGW" ]; then
              echo "Detaching and deleting Internet Gateway: $IGW"
              aws ec2 detach-internet-gateway --internet-gateway-id $IGW --vpc-id $VPC_ID
              aws ec2 delete-internet-gateway --internet-gateway-id $IGW
            fi

            # Delete Subnets
            echo "Checking Subnets"
            SUBNETS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[*].SubnetId' --output text)
            for subnet in $SUBNETS; do
              echo "Deleting Subnet: $subnet"
              aws ec2 delete-subnet --subnet-id $subnet
            done

            # Delete Route Tables
            echo "Checking Route Tables"
            RTS=$(aws ec2 describe-route-tables --filters "Name=vpc-id,Values=$VPC_ID" --query 'RouteTables[?Associations[0].Main != `true`].RouteTableId' --output text)
            for rt in $RTS; do
              echo "Deleting Route Table: $rt"
              aws ec2 delete-route-table --route-table-id $rt
            done

            # Delete Security Groups (except default)
            echo "Checking Security Groups"
            SGS=$(aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$VPC_ID" --query 'SecurityGroups[?GroupName!=`default`].GroupId' --output text)
            for sg in $SGS; do
              echo "Deleting Security Group: $sg"
              aws ec2 delete-security-group --group-id $sg
            done

            # Finally, delete the VPC
            echo "Deleting VPC: $VPC_ID"
            aws ec2 delete-vpc --vpc-id $VPC_ID
          }

          # Get VPC ID and clean it
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=${CLUSTER_NAME}-vpc" --query 'Vpcs[*].VpcId' --output text)
          if [ ! -z "$VPC_ID" ]; then
            echo "Found VPC: $VPC_ID"
            clean_vpc $VPC_ID
          fi
        continue-on-error: true

      - name: Check and Clean Existing Resources
        run: |
          # Check for existing node groups
          if NODEGROUPS=$(aws eks list-nodegroups --cluster-name $CLUSTER_NAME --region ${{ env.AWS_REGION }} --query 'nodegroups[*]' --output text 2>/dev/null); then
            echo "Found existing node groups: $NODEGROUPS"
            for ng in $NODEGROUPS; do
              echo "Deleting node group: $ng"
              aws eks delete-nodegroup \
                --cluster-name $CLUSTER_NAME \
                --nodegroup-name $ng \
                --region ${{ env.AWS_REGION }}
              
              echo "Waiting for node group deletion..."
              aws eks wait nodegroup-deleted \
                --cluster-name $CLUSTER_NAME \
                --nodegroup-name $ng \
                --region ${{ env.AWS_REGION }}
            done
          fi

          # Check for existing cluster and delete it
          if aws eks describe-cluster --name $CLUSTER_NAME --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "Deleting existing EKS cluster: $CLUSTER_NAME"
            aws eks delete-cluster \
              --name $CLUSTER_NAME \
              --region ${{ env.AWS_REGION }}
            
            echo "Waiting for cluster deletion..."
            aws eks wait cluster-deleted \
              --name $CLUSTER_NAME \
              --region ${{ env.AWS_REGION }}
          fi
        continue-on-error: true

      - name: Check and Clean Existing EKS Resources
        run: |
          # Function to wait for cluster deletion
          wait_for_cluster_deletion() {
            local cluster_name=$1
            echo "Waiting for cluster deletion to complete..."
            while true; do
              if ! aws eks describe-cluster --name $cluster_name --region ${{ env.AWS_REGION }} 2>/dev/null; then
                echo "Cluster deleted successfully"
                break
              fi
              echo "Still waiting for cluster deletion..."
              sleep 30
            done
          }

          # Check for existing node groups and delete them
          if NODEGROUPS=$(aws eks list-nodegroups --cluster-name $CLUSTER_NAME --region ${{ env.AWS_REGION }} --query 'nodegroups[*]' --output text 2>/dev/null); then
            echo "Found existing node groups: $NODEGROUPS"
            for ng in $NODEGROUPS; do
              echo "Deleting node group: $ng"
              aws eks delete-nodegroup \
                --cluster-name $CLUSTER_NAME \
                --nodegroup-name $ng \
                --region ${{ env.AWS_REGION }}
              
              echo "Waiting for node group deletion..."
              aws eks wait nodegroup-deleted \
                --cluster-name $CLUSTER_NAME \
                --nodegroup-name $ng \
                --region ${{ env.AWS_REGION }}
            done
          fi

          # Check for existing cluster and delete it
          if aws eks describe-cluster --name $CLUSTER_NAME --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "Deleting existing EKS cluster: $CLUSTER_NAME"
            aws eks delete-cluster \
              --name $CLUSTER_NAME \
              --region ${{ env.AWS_REGION }}
            
            wait_for_cluster_deletion $CLUSTER_NAME
          fi
        continue-on-error: true

      - name: Check and Clean Existing IAM Roles
        run: |
          # Function to detach and delete role policies
          delete_role() {
            local ROLE_NAME=$1
            echo "Checking role: $ROLE_NAME"
            
            # List and detach attached policies
            for ARN in $(aws iam list-attached-role-policies --role-name $ROLE_NAME --query 'AttachedPolicies[*].PolicyArn' --output text 2>/dev/null); do
              echo "Detaching policy: $ARN from role: $ROLE_NAME"
              aws iam detach-role-policy --role-name $ROLE_NAME --policy-arn $ARN
            done
            
            # Delete inline policies if any
            for POLICY in $(aws iam list-role-policies --role-name $ROLE_NAME --query 'PolicyNames[*]' --output text 2>/dev/null); do
              echo "Deleting inline policy: $POLICY from role: $ROLE_NAME"
              aws iam delete-role-policy --role-name $ROLE_NAME --policy-name $POLICY
            done
            
            # Delete the role
            echo "Deleting role: $ROLE_NAME"
            aws iam delete-role --role-name $ROLE_NAME
          }

          # Delete cluster role
          CLUSTER_ROLE="${CLUSTER_NAME}-cluster-role"
          if aws iam get-role --role-name $CLUSTER_ROLE 2>/dev/null; then
            delete_role $CLUSTER_ROLE
          fi

          # Delete node role
          NODE_ROLE="${CLUSTER_NAME}-node-role"
          if aws iam get-role --role-name $NODE_ROLE 2>/dev/null; then
            delete_role $NODE_ROLE
          fi
        continue-on-error: true

      - name: Terraform Init
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: |
          terraform init \
            -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
            -backend-config="key=${{ github.event.inputs.environment }}/terraform.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}"

      - name: Terraform Plan
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: |
          terraform plan \
            -var="environment=${{ github.event.inputs.environment }}" \
            -var="cluster_name=$CLUSTER_NAME" \
            -out=tfplan

      - name: Terraform Apply
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: |
          terraform apply -auto-approve -input=false tfplan

      - name: Verify Cluster
        if: success()
        run: |
          echo "Waiting for cluster to be ready..."
          aws eks wait cluster-active \
            --name $CLUSTER_NAME \
            --region ${{ env.AWS_REGION }}

          echo "Configuring kubectl..."
          aws eks update-kubeconfig \
            --name $CLUSTER_NAME \
            --region ${{ env.AWS_REGION }}

          echo "Waiting for nodes to be ready..."
          kubectl wait --for=condition=ready nodes --all --timeout=10m

      - name: Output Cluster Info
        if: success()
        run: |
          echo "Cluster Info:"
          kubectl cluster-info

          echo "Node Status:"
          kubectl get nodes

          echo "Airflow Status:"
          kubectl get pods -n airflow
          echo "Airflow Webserver URL:"
          kubectl get svc -n airflow airflow-webserver
